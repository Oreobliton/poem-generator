Je veux faire un générateur de texte en me basant sur un modèle existant (surement RNN ou une optimisation de chaine de markov)
Pour ça je me pose beaucoup de questions.
D'un point de vu purement technique je sais au moins deux choses :
    - je veux le coder en Python
    - je veux traiter du texte en français
Déjà y'a la partie cleaning et accumulation de data. Plusieurs axes s'offrent à moi : 
    Les poèmes 
    Les chansons (parole.net) 
    Les horoscopes
C'est un choix un peu secondaire, la question principale est dans le choix du modèle pour l'instant.
Je suis en train de lire cet article (de blog) pour avoir des idées d'où aller :
https://towardsdatascience.com/exploring-wild-west-of-natural-language-generation-from-n-gram-and-rnns-to-seq2seq-2e816edd89c6

Le modèle n-gram est représentable avec mon approche actuelle (chaine de markov)
https://towardsdatascience.com/text-generation-using-n-gram-model-8d12d9802aa0

Je sais que je peux l'améliorer en utilisant le Q-Learning
Cet article explique bien le concept de q learning et les applications qu'il peut avoir :
https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/
Je vois comment l'implémenter après il faut que le résultat soit vraiment meilleur que ce que 
je faisais avec mes n-grammes de base.
Il y'a d'autres sources dans la description : 
https://pathak22.github.io/noreward-rl/
https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html
https://ai.googleblog.com/2019/02/learning-to-generalize-from-sparse-and.html



Je pense qu'on va faire 3 modèles : 
    Markov de base
    Markov + Q learning
    RNN
  
----->  On va les comparer avec le résultat de GPT2 ou 3
        On peut difficilement quantifier de manière automatique
        Donc il faudra comparer un peu à la main et tricher